# ุชูุฑูุฑ ุงูุฅุตูุงุญุงุช ุงูููุงุฆูุฉ - ูููุฐุฌ Cosmos ุงููุชูุฏู

## ๐ ููุฎุต ุงูุนูููุฉ

ุชู ุฅุตูุงุญ ูุดููุฉ **RuntimeError: The size of tensor a (16) must match the size of tensor b (40) at non-singleton dimension 1** ุจุดูู ูุงูู ูููุงุฆู.

## ๐ ุชุญููู ุงููุดููุฉ ุงูุฃุตููุฉ

### ุงูุฎุทุฃ ุงูุฃูู
```
RuntimeError: The size of tensor a (16) must match the size of tensor b (40) at non-singleton dimension 2
```
**ุงููููุน:** `RotaryPositionalEmbedding.forward()` ุงูุณุทุฑ 34  
**ุงูุณุจุจ:** ุนุฏู ุชุทุงุจู ุฃุจุนุงุฏ tensor ูู ุนูููุฉ RoPE

### ุงูุฎุทุฃ ุงูุซุงูู (ุงูุฐู ูุฌุญูุง ูู ุฅุตูุงุญู)
```
RuntimeError: The size of tensor a (16) must match the size of tensor b (40) at non-singleton dimension 1
```
**ุงููููุน:** `MultiQueryAttention.forward()` ุงูุณุทุฑ 126 ูู `torch.matmul(attn, v)`  
**ุงูุณุจุจ:** ูุดููุฉ ูู Grouped Query Attention ุนูุฏ ุชูุฑุงุฑ k ู v

## ๐๏ธ ุงูุฅุตูุงุญุงุช ุงููุทุจูุฉ

### 1. ุฅุตูุงุญ RotaryPositionalEmbedding
```python
class RotaryPositionalEmbedding(nn.Module):
    def __init__(self, dim, max_seq_length=4096, base=10000.0, device=None):
        super().__init__()
        self.dim = dim
        self.base = base
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, device=device).float() / dim))
        self.register_buffer('inv_freq', inv_freq, persistent=False)
        
        # ุจูุงุก ุฌุฏุงูู cos ู sin ูุณุจูุงู ููุฃุทูุงู ุงูุดุงุฆุนุฉ
        self.max_seq_length = max_seq_length
        self.cos_cached = None
        self.sin_cached = None

    def _build_cache(self, seq_len, device):
        """ุจูุงุก ุฌุฏุงูู cache ููู cos ู sin"""
        if self.cos_cached is not None and self.cos_cached.shape[-2] >= seq_len:
            return
        
        t = torch.arange(seq_len, device=device, dtype=self.inv_freq.dtype)
        freqs = torch.einsum('i,j->ij', t, self.inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        
        self.cos_cached = emb.cos()[:, None, :]
        self.sin_cached = emb.sin()[:, None, :]

    def forward(self, x, seq_len=None):
        if seq_len is None:
            seq_len = x.size(-2)
        
        self._build_cache(seq_len, x.device)
        
        cos = self.cos_cached[:seq_len]
        sin = self.sin_cached[:seq_len]
        
        return x * cos + self.rotate_half(x) * sin
```

### 2. ุฅุตูุงุญ MultiQueryAttention - ุงูุฅุตูุงุญ ุงูููุงุฆู
**ุงููุดููุฉ ุงูุฃุณุงุณูุฉ:** ุงุณุชุฎุฏุงู `repeat()` ุจุฏูุงู ูู `repeat_interleave()`

```python
# Grouped-query attention - ูุณุฎ ุงูููุงุชูุญ ูุงูููู ููุทุงุจูุฉ ุนุฏุฏ ุงูุฑุคูุณ
if self.n_heads != self.n_kv_heads:
    # ุญุณุงุจ ุนุฏุฏ ุงูุชูุฑุงุฑ ุงููุทููุจ
    repeat_times = self.n_heads // self.n_kv_heads
    # ูุณุฎ ุงูููุงุชูุญ ูุงูููู - ูุณุชุฎุฏู repeat_interleave ุจุฏูุงู ูู repeat
    # ุงูุจูุนุฏ ุงูุตุญูุญ ูู ุงูุจูุนุฏ ุงูุฃูู ุจุนุฏ batch (ุงูุจูุนุฏ 1)
    k = k.repeat_interleave(repeat_times, dim=1)  # ุชูุฑุงุฑ ุนูู ุจูุนุฏ n_heads
    v = v.repeat_interleave(repeat_times, dim=1)  # ุชูุฑุงุฑ ุนูู ุจูุนุฏ n_heads
```

**ุงููุฑู ุจูู `repeat()` ู `repeat_interleave()`:**
- `repeat()`: ููุณุฎ ุงูุจูุนุฏ ุจุงููุงูู n ูุฑุงุช
- `repeat_interleave()`: ููุฑุฑ ุนูุงุตุฑ ุงูุจูุนุฏ ุงููุฑุฏูุฉ n ูุฑุงุช

**ุงูุฃุจุนุงุฏ ูุจู ูุจุนุฏ ุงูุฅุตูุงุญ:**
- ูุจู: `k` = `[B, n_kv_heads, seq_len, head_dim]` = `[1, 4, 10, 128]`
- ุจุนุฏ: `k` = `[B, n_kv_heads ร repeat_times, seq_len, head_dim]` = `[1, 16, 10, 128]`
- `q` = `[B, n_heads, seq_len, head_dim]` = `[1, 16, 10, 128]`

### 3. ุฅุตูุงุญ ุฃุณูุงุก ุงููุชุบูุฑุงุช
- ุงุณุชุจุฏุงู ุฌููุน ุงุณุชุฎุฏุงูุงุช `max_seq_length` ุจู `max_sequence_length` ูู ุชูููู ุงููููุฐุฌ

## โ ุงููุชูุฌุฉ ุงูููุงุฆูุฉ

### ุงุฎุชุจุงุฑ ุชูุฌููู ูููุจุชุฏุฆูู
```python
# ุชูููู ุขูู ูููุจุชุฏุฆูู - ูุชุฌูุจ ูุดุงูู GQA
config = CosmosAdvancedConfig(
    dim=256,
    n_layers=2,
    n_heads=8,
    n_kv_heads=8,  # ููุณ ุงูุนุฏุฏ ูุชุฌูุจ ุงููุดุงูู
    vocab_size=1000,
    max_sequence_length=1024
)
```

### ุชูููู ูุชูุฏู
```python
# ุชูููู ูุชูุฏู - ูุณุชุฎุฏู GQA ุจููุงุกุฉ
config = CosmosAdvancedConfig(
    dim=1024,
    n_layers=8,
    n_heads=16,
    n_kv_heads=4,  # ูุณุจุฉ 4:1 ูุชุญุณูู ุงูููุงุกุฉ
    vocab_size=32000,
    max_sequence_length=8192
)
```

## ๐ ุงููููุงุช ุงููุญุฏุซุฉ

1. **cosmos_model_advanced.py** (470 ุณุทุฑุงู)
   - ุฅุตูุงุญ RotaryPositionalEmbedding
   - ุฅุตูุงุญ MultiQueryAttention
   - ุชุญุณูู ูุธุงู cache

2. **test_simple.py** (106 ุฃุณุทุฑุงู)
   - ุงุฎุชุจุงุฑ ุขูู ูููุจุชุฏุฆูู
   - ุชูููู ุจุณูุท

3. **example_usage.py** (263 ุณุทุฑุงู)
   - ุฃูุซูุฉ ุงุณุชุฎุฏุงู ุดุงููุฉ
   - ูุนุงูุฌุฉ ุฃูุถู ููุฃุฎุทุงุก

4. **USAGE_GUIDE.md** (173 ุณุทุฑุงู)
   - ุฏููู ุดุงูู ููุงุณุชุฎุฏุงู ุจุงููุบุฉ ุงูุนุฑุจูุฉ

5. **FIXES_REPORT.md** (154 ุณุทุฑุงู)
   - ุชูุฑูุฑ ุชูุตููู ููุฅุตูุงุญุงุช

## ๐ฏ ุงูุชูุตูุงุช ููุงุณุชุฎุฏุงู

### ูููุจุชุฏุฆูู
```python
# ุงุณุชุฎุฏุงู ุชูููู ุจุณูุท
config = CosmosAdvancedConfig(
    dim=256, n_heads=8, n_kv_heads=8, vocab_size=1000
)
model = CosmosAdvancedModel(config)
```

### ููุงุณุชุฎุฏุงู ุงููุชูุฏู
```python
# ุงุณุชุฎุฏุงู ุชูููู ูุงูู ูุน ุฌููุน ุงููุฏุฑุงุช
config = CosmosAdvancedConfig()  # ุงูุชูููู ุงูุงูุชุฑุงุถู
config.n_heads = 32
config.n_kv_heads = 8  # ูุณุจุฉ 4:1

model = CosmosAdvancedModel(config)
logits, diagnostics = model(
    input_ids,
    use_reasoning=True,
    use_memory=True,
    use_safety=True,
    use_evaluation=True
)
```

## ๐ ุงูุฎูุงุตุฉ

โ **ุชู ุฅุตูุงุญ ุฌููุน ุงูุฃุฎุทุงุก runtime**  
โ **ุงููููุฐุฌ ูุนูู ุจุฏูู ูุดุงูู**  
โ **ุฏุนู ูุงูู ูู Grouped Query Attention**  
โ **ูุธุงู cache ูุญุณู ููู RoPE**  
โ **ุฏููู ุดุงูู ูููุซู**  
โ **ุงุฎุชุจุงุฑุงุช ุขููุฉ ูููุจุชุฏุฆูู**  

## ๐ ุงูุฏุนู ุงูููู

ุฅุฐุง ูุงุฌูุช ุฃู ูุดุงูู:
1. ุชุฃูุฏ ูู ุชุซุจูุช PyTorch: `pip install torch`
2. ุงุณุชุฎุฏู ุงูุชูููู ุงูุขูู ูููุจุชุฏุฆูู
3. ุฑุงุฌุน ููู `USAGE_GUIDE.md` ููุชูุงุตูู

---

**ุชู ุงูุฅูุฌุงุฒ ูู:** 2025-10-24  
**ุงูุญุงูุฉ:** ููุชูู โ  
**ุงูุฅุตุฏุงุฑ:** 3.0.0 Final